{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import sys \n",
    "import matplotlib \n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:17:26) [MSC v.1500 64 bit (AMD64)]\n",
      "Pandas version 0.20.1\n",
      "Matplotlib version 2.0.2\n"
     ]
    }
   ],
   "source": [
    "print('Python version ' + sys.version)\n",
    "print('Pandas version ' + pd.__version__)\n",
    "print('Matplotlib version ' + matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Joins Coarse-Discourse annotations with Reddit data via Reddit API.'''\n",
    "# Copyright 2017 Google Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import __future__\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import praw\n",
    "\n",
    "'''\n",
    "Requirements: PRAW python library - https://praw.readthedocs.io/en/latest/index.html\n",
    "              current version: 4.4.0\n",
    "\n",
    "Date: 04/02/2017\n",
    "\n",
    "This script adds fields to the coarse-discourse dataset for each comment including the \n",
    "text of the comment and the author. The information is taken from the Reddit API.\n",
    "\n",
    "You can augment this script to gather other information about the comment such as upvote \n",
    "or downvote count or about the comment author, such as their karma, from the fields\n",
    "that the Reddit API provides.\n",
    "\n",
    "\n",
    "Note:\n",
    "\n",
    "There may be discrepancies due to changes made between when the coarse-discourse\n",
    "data was collected (August 2016) and when you are accessing the API, such as missing \n",
    "comments or comment text.\n",
    "\n",
    "You should be able to overcome discrepancies by downloading the full Reddit dump from \n",
    "the beginning up until 09/2016 found in various places such as: \n",
    "\n",
    "https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/\n",
    "https://archive.org/details/2015_reddit_comments_corpus\n",
    "https://bigquery.cloud.google.com/dataset/fh-bigquery:reddit_posts\n",
    "\n",
    "and using that data instead of the Reddit API to collect the comment texts and author names.\n",
    "'''\n",
    "\n",
    "# Replace below with information provided to you by Reddit when registering your script\n",
    "reddit = praw.Reddit(client_id=\"QutcH0SZqEwjBw\",\n",
    "                     client_secret=\"BDIt_Tif5Eo5asnYBC-lZdQ8Q5E\",\n",
    "                     user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\\\n",
    "                     AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36\")\n",
    "\n",
    "with open('coarse_discourse_dataset.json') as jsonfile:\n",
    "    lines = jsonfile.readlines()\n",
    "    dump_with_reddit = open('coarse_discourse_dump_reddit.json', 'w')\n",
    "\n",
    "    for line in lines:\n",
    "        reader = json.loads(line)\n",
    "        print(reader['url'])\n",
    "\n",
    "        submission = reddit.submission(url=reader['url'])\n",
    "\n",
    "        # Annotators only annotated the 40 \"best\" comments determined by Reddit\n",
    "        submission.comment_sort = 'best'\n",
    "        submission.comment_limit = 40\n",
    "\n",
    "        post_id_dict = {}\n",
    "\n",
    "        for post in reader['posts']:\n",
    "            post_id_dict[post['id']] = post\n",
    "\n",
    "        try:\n",
    "            full_submission_id = 't3_' + submission.id\n",
    "            if full_submission_id in post_id_dict:\n",
    "                post_id_dict[full_submission_id]['body'] = submission.selftext\n",
    "\n",
    "                # For a self-post, this URL will be the same URL as the thread.\n",
    "                # For a link-post, this URL will be the link that the link-post is linking to.\n",
    "                post_id_dict[full_submission_id]['url'] = submission.url\n",
    "                if submission.author:\n",
    "                    post_id_dict[full_submission_id]['author'] = submission.author.name\n",
    "\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            for comment in submission.comments.list():\n",
    "                full_comment_id = 't1_' + comment.id\n",
    "                if full_comment_id in post_id_dict:\n",
    "                    post_id_dict[full_comment_id]['body'] = comment.body\n",
    "                    if comment.author:\n",
    "                        post_id_dict[full_comment_id]['author'] = comment.author.name\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Error %s' % (e))\n",
    "\n",
    "        found_count = 0\n",
    "        for post in reader['posts']:\n",
    "            if not post.has_key('body'):\n",
    "                print(\"Can't find %s in URL: %s\" % (post['id'], reader['url']))\n",
    "            else:\n",
    "                found_count += 1\n",
    "\n",
    "        print('Found %s posts out of %s' % (found_count, len(reader['posts'])))\n",
    "\n",
    "        dump_with_reddit.write(json.dumps(reader) + '\\n')\n",
    "\n",
    "        # To keep within Reddit API limits\n",
    "        time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_json?\n",
    "# path\n",
    "jsonloc = r'C:\\Users\\Sophia\\DTI_semifinal\\coarse-discourse\\coarse_discourse_dataset.json'\n",
    "\n",
    "# take a look at the json annotation file\n",
    "df = pd.read_json(jsonloc, lines = True)\n",
    "df.dtypes\n",
    "print df.head()\n",
    "df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "('...', 1000, 1000)\n",
      "('...', 2000, 2000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 3000, 3000)\n",
      "('...', 4000, 4000)\n",
      "('...', 4000, 4000)\n",
      "('...', 5000, 5000)\n",
      "('...', 5000, 5000)\n",
      "('...', 5000, 5000)\n",
      "('...', 6000, 6000)\n",
      "('...', 6000, 6000)\n",
      "('...', 6000, 6000)\n",
      "('...', 7000, 7000)\n",
      ".\n",
      "('...', 8000, 8000)\n",
      "('...', 8000, 8000)\n",
      "('...', 8000, 8000)\n",
      "('...', 9000, 9000)\n",
      "('...', 9000, 9000)\n",
      "('...', 9000, 9000)\n",
      "('...', 9000, 9000)\n",
      "('...', 9000, 9000)\n",
      "('...', 9000, 9000)\n",
      "('...', 9000, 9000)\n",
      "('...', 10000, 10000)\n",
      "('...', 11000, 11000)\n",
      "('...', 11000, 11000)\n",
      "('...', 11000, 11000)\n",
      "('...', 11000, 11000)\n",
      "('...', 12000, 12000)\n",
      "('...', 13000, 13000)\n",
      "('...', 13000, 13000)\n",
      "('...', 14000, 14000)\n",
      ".\n",
      "('...', 15000, 15000)\n",
      "('...', 15000, 15000)\n",
      "('...', 16000, 16000)\n",
      "('...', 17000, 17000)\n",
      "('...', 18000, 18000)\n",
      "('...', 19000, 19000)\n",
      "('...', 19000, 19000)\n",
      "('...', 20000, 20000)\n",
      ".\n",
      "('...', 21000, 21000)\n",
      "('...', 22000, 22000)\n",
      "('...', 22000, 22000)\n",
      "('...', 23000, 23000)\n",
      "('...', 24000, 24000)\n",
      "('...', 24000, 24000)\n",
      "('...', 24000, 24000)\n",
      "('...', 24000, 24000)\n",
      "('...', 25000, 25000)\n",
      "('...', 26000, 26000)\n",
      "('...', 26000, 26000)\n",
      "('...', 26000, 26000)\n",
      "('...', 26000, 26000)\n",
      "('...', 26000, 26000)\n",
      ".\n",
      "('...', 27000, 27000)\n",
      "('...', 27000, 27000)\n",
      "('...', 27000, 27000)\n",
      "('...', 27000, 27000)\n",
      "('...', 28000, 28000)\n",
      "('...', 29000, 29000)\n",
      "('...', 30000, 30000)\n",
      "('...', 31000, 31000)\n",
      "('...', 32000, 32000)\n",
      "('...', 32000, 32000)\n",
      "('...', 32000, 32000)\n",
      ".\n",
      "('...', 33000, 33000)\n",
      "('...', 34000, 34000)\n",
      "('...', 35000, 35000)\n",
      "('...', 36000, 36000)\n",
      "('...', 37000, 37000)\n",
      "('...', 38000, 38000)\n",
      "('...', 38000, 38000)\n",
      ".\n",
      "('...', 39000, 39000)\n",
      "('...', 39000, 39000)\n",
      "('...', 39000, 39000)\n",
      "('...', 39000, 39000)\n",
      "('...', 40000, 40000)\n",
      "('...', 40000, 40000)\n",
      "('...', 40000, 40000)\n",
      "('...', 40000, 40000)\n",
      "('...', 40000, 40000)\n",
      "('...', 41000, 41000)\n",
      "('...', 42000, 42000)\n",
      "('...', 43000, 43000)\n",
      "('...', 44000, 44000)\n",
      "('...', 45000, 45000)\n",
      "('...', 45000, 45000)\n",
      ".\n",
      "('...', 46000, 46000)\n",
      "('...', 47000, 47000)\n",
      "('...', 48000, 48000)\n",
      "('...', 49000, 49000)\n",
      "('...', 50000, 50000)\n",
      "('...', 50000, 50000)\n",
      "('...', 50000, 50000)\n",
      "('...', 51000, 51000)\n",
      ".\n",
      "('...', 52000, 52000)\n",
      "('...', 53000, 53000)\n",
      "('...', 53000, 53000)\n",
      "('...', 54000, 54000)\n",
      "('...', 54000, 54000)\n",
      "('...', 55000, 55000)\n",
      "('...', 56000, 56000)\n",
      "('...', 56000, 56000)\n",
      "('...', 56000, 56000)\n",
      "('...', 57000, 57000)\n",
      "('...', 57000, 57000)\n",
      "('...', 57000, 57000)\n",
      "('...', 57000, 57000)\n",
      "('...', 57000, 57000)\n",
      "('...', 57000, 57000)\n",
      "('...', 57000, 57000)\n",
      ".\n",
      "('...', 58000, 58000)\n",
      "('...', 59000, 59000)\n",
      "('...', 60000, 60000)\n"
     ]
    }
   ],
   "source": [
    "# %load prepair_data.py\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "comments = []\n",
    "labels = []\n",
    "\n",
    "label_categories = [\"question\", \"answer\", \"announcement\",\n",
    "                    \"appreciation\",\"agreement\", \"elaboration\",\n",
    "                    \"disagreement\", \"humor\", \"negativereaction\"]\n",
    "\n",
    "label_map = dict((s, i) for i, s in enumerate(label_categories))\n",
    "\n",
    "with open('coarse_discourse_dump_reddit.json', 'r') as f:\n",
    "    for line_idx, line in enumerate(f):\n",
    "        if line_idx >= 9482: # last line is incomplete\n",
    "            break\n",
    "\n",
    "        df = pd.read_json(line.encode('utf-8').strip())\n",
    "        crt_post_df = df[\"posts\"]\n",
    "        for post_idx in range(len(crt_post_df)):\n",
    "            annotation_df = crt_post_df[post_idx][\"annotations\"]\n",
    "            # check if all annotation agrees\n",
    "            c = Counter(annotation_df[i][\"main_type\"] for i in range(len(annotation_df)))\n",
    "            vote, count = c.most_common()[0]\n",
    "            agree = count == 3 and vote in label_map\n",
    "            if agree:\n",
    "                if \"body\" in crt_post_df[post_idx]: # and annotation_df[0][\"main_type\"] in label_map\n",
    "                    comments.append(crt_post_df[post_idx][\"body\"].encode('utf-8'))\n",
    "                    labels.append(label_map[vote])\n",
    "            if len(comments) % 1000 == 0:\n",
    "                print ('...', len(comments), len(labels))\n",
    "        if line_idx % 1000 == 0:\n",
    "            print('.')\n",
    "        \n",
    "\n",
    "assert len(comments)==len(labels), \"number of comments and labels should match\"\n",
    "\n",
    "# random split train and test\n",
    "perm_idx = np.random.permutation(len(comments))\n",
    "ntrain = int(0.8*len(comments))\n",
    "ntest = len(comments) - ntrain\n",
    "train_idx = perm_idx[:ntrain]\n",
    "test_idx = perm_idx[ntrain:]\n",
    "\n",
    "with open('train_comments.csv', 'w') as f:\n",
    "    for idx in train_idx:\n",
    "        c = comments[idx]\n",
    "        c = c.replace('\\n', ' ').strip()\n",
    "        f.write(\"{},{}\\n\".format(c, labels[idx]))\n",
    "\n",
    "with open('test_comments.csv', 'w') as f:\n",
    "    for idx in test_idx:\n",
    "        c = comments[idx]\n",
    "        c = c.replace('\\n', ' ').strip()\n",
    "        f.write(\"{},{}\\n\".format(c, labels[idx]))\n",
    "        \n",
    "with open('comments.csv', 'w') as f:\n",
    "    for idx in range(len(comments)):\n",
    "        c = comments[idx]\n",
    "        c = c.replace('\\n', ' ').strip()\n",
    "        f.write(\"{},{}\\n\".format(c, labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
