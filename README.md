## Course discourse classification
### Background
I anayzed a dataset presented in Zhang, Culterton and Paritosh (2017) (also see https://github.com/google-research-datasets/coarse-discourse).  The authors have collected large corpus of threads of discussion that involve back and force comments from Reddit (hosting news aggregation, web content rating, and discussion website).  The comments were manually labeled with nine categories (also known as speech/discourse act such as Q&A, disagreement, and agreement).   The goal of this study is expanding the scope of current speech act classification problems that mainly focused on Q&A or considered only communities for technical support.  Therefore classification for a richer set of categories is an interesting and challenging step forward in this type of question. 

The main goal of my study is to classify on-line discussion into finer categories developed in Zhang et al. (2017) using different text feature extraction and learning methods.  The dataset I choose to use include over 9000 Reddit threats with a total of 60000 individual labeled comments.  

### Methods
I downloaded the metadata about 9483 sampled threads (e.g. author, relation among posts in a thread, and annotators’ ID and labels from each of the three annotators), and assembled thread contents using Reddit API (a script has been provided by Zhang et al.).  At current step of proposal, I only focused on the thread contents (comments), and did not start analyzing the metadata which also contain informative features for classification.  The ultimate goal is to use both the text as well as characterization from metadata for the classification taste.   I extracted individual comments from each of 9783 threads with over 100,000 comments, and I selected the comments with completely consistent labels from all the three annotators.  Furthermore, I excluded the ‘others’ category.  Finally over 60,000 individual comments from the 9483 threads were analyzed. 
	
Three feature extraction methods have been applied, bag of words (one-gram, 2-gram, and 3-gram) and term frequency-inverse document frequency (tf-idf), as well as word2vector.  The dataset has been random split, and 80% were used as training samples and 20% as testing samples.  Logistic regression, random forest, as well as linear Supportive Vector Machine (linear SVM) have been applied to classify the comments into nine categories.  Predication accuracy on the training set, and 3-fold cross validation was applied to select best model, and accuracy was also reported for each combination of feature extraction and learning methods.  

### Results and future work
Prediction accuracy on train and test dataset is presented in Plot 1.  Linear SVM with Tf-idf generated the best result: prediction accuracy 68.25%.   From the random forest models, the high-ranked features for classification include words such as "thank you", "thanks for", "what", "how" etc. (Plot 2). It is helpful to examine the category-specific features in feature modification and extraction.  Importantly I did not include any features based on post relation such as "weather the post is the start of the conversation", and "if the post replies to the original author", "how wide-spread of the posts".  Characterizing post relation will be a significant improvement of the study.  Moreover, appropriate up-sampling (e.g. bootstrap sampling) on minority categories or aggregation should be applired to improved prediction accuracy for minorigy categories. 
 

